### Example of ocean model architecture. 
### PSyKAl and Block Load-Balancing approach.
### Using modern Fortran (2003 at least).

### Shallow Water only example.

Structure:

```
control         - Algorithm Layer       : work with grid_type and ocean_type

core, interface - Parallel System Layer : data types and transfer data to kernel (core/kernel_interface.f90 - main envokes for kernels)

kernel          - Kernel Layer          : work with 1D, 2D and 3D fortran arrays, per one block

gpu             - Kernel and Algorithm Layers for CUDA

macros          - Main configuration for model (OpenMP modes, MPI, GPU)

```

```
service   - Specific utills (grid and mask construction, etc)

shared    - Common utills (MPI, etc)

configs   - Common configs

tools     - Shell for legacy

legacy    - Original code from INMOM
```

I/O, Time Manager, Configs - from ocean model INMOM


mpp_macros.fi:
#define _MPP_KERNEL_TIMER_ON_
    Опция меряет время выполнения ядер каждым потоком. У каждого потока прописывается id по имени, функция get_kernel_id(). Функционал старый и больше не поддерживается. Соответственно может не работать на данный момент.

#define _MPP_SORTED_BLOCKS_
    Опция включает сортировку блоков по их весу. В перечислении do k = 1, domain%bcount идут сначала номера блоков с большим весом. Опция полезна при распараллеливании по потокам: в перечислении блоков используется директива !$omp do private(k) schedule(static, 1) и потоки разбирают блоки с большей загруженностью, что обеспечивает балансировку нагрузки по потокам. Опция была протестирована, прирост в производительности есть.

#define _MPP_NO_PARALLEL_MODE_
    Опция отключает распараллеливание по потокам. За исключением пересылок в MPI.

#define _MPP_BLOCK_MODE_
    Опция включает распараллеливание по потокам в режиме task-based. Основной режим распараллеливания по потокам. Каждому потоку ставится в соответствие набор блоков для вычисления. Пересылки в MPI также распараллелены. Особое внимание уделяется first-touch policy и целостность кэшей в NUMA системах. Правда, ускорение на NUMA системах работает не стабильно, возможно потому что нужно сбрасывать кэши перед началом расчета, чтобы при инициализации данных память закреплялась. Также, нету закрепления потоков в коде, предполагается использовать OMP_PINNED (?). TODO: configs from cluster.

#define _MPP_HYBRID_BLOCK_MODE_
    Не работает. Работает как _MPP_BLOCK_MODE_, но без синхронизаций MPI. Есть основа для перекрытия вычислений/синхронизаций, но пока не реализовано. Были попытки реализации, но наблюдалось только замедление (может из-за малого размера системы). Поэтому сейчас полностью выкинуто из текущей версии кода.

#define _MPP_MAX_SIMUL_SYNCS_
    Количество одновременных пересылок данных. Например, идея одновременно пересылать поле скоростей и поле уровня. Опция нужна при работе _MPP_HYBRID_BLOCK_MODE_, т.к. там идет отдельная пересылка границы и внутренних точек и действительно есть возможность пересылать некоторые поля одновременно. Сейчас не поддержано. Задет только размеры буфферов в MPI, но значение отличное от 1 ставить нету смысла.

#define _DBG_MEM_PROFILE_
    Опция включает профилирование по памяти. При каждом вызове init_data* внутри считается количество выделяемой памяти.

#define _GPU_MODE_
    Общая опция для включения GPU расчета. Инициализируется память на девайсе, включается код под графические процессоры (компилировать нужно уже с помощью nvfortran). Без модификаторов опция работает как GPU + MPI, т.е. с пересылками между девайсом и хостом. На каждый блок вызывается свое CUDA ядро со своим stream потоком.

#define _GPU_FULL_
    Модификатор к опции _GPU_MODE_. Включает расчет полностью на GPU без синхронизаций с CPU. Все данные всегда на GPU.

#define _GPU_ASYNC_
    Модификатор к опции _GPU_MODE_. Включает асихронную пересылку данных между хостом и девайсом. Опция имеет смысл при использовании больше одного блока на процесс. В таком случае на современных видеокартах можно наблюдать перекрытие вычислений CUDA ядер с операциями копирования с хоста на девайс и обратно.

sw.par:
    Возможность включать трасеры, перенос, диффузию. Задает физические параметры и т.д.

basin.par
    Возможность задавать сетку.

parallel.par
    Возможность задавать параллельную конфигурацию, блоки, виды разбиения на блоки и т.д.

ocean_run.par
    Задает шаг по времени, длительность расчета, запись результатов и т.д.
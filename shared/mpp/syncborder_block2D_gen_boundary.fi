!------------------------------------------------------------------------------
! Generic for 2D sync routine:
! subroutine syncborder_data2D_TYPE(domain, data2d)
!        type(data2D_real8_type), intent(inout) :: data2d
!        type(domain_type), intent(in) :: domain
!        ...
! end subroutine
!
! Must be specified: 
!   _MPI_TYPE_
!
!------------------------------------------------------------------------------

integer :: reqst
integer :: k, kk, ierr
integer :: sync_dir(8)
integer :: k_dir(8)
integer :: rank_dir(8)
integer :: nxs, nxe, nys, nye     ! Boundary points
integer :: buf_size
integer :: rk                     ! Local index of buffers for rank, see sync_map_rank

real(wp8) :: t_local

!$omp master

call start_timer(t_local)

if (sync_tag == 1) then
    sync_count_send_recv = 0
endif

!$omp end master

! Prepare MPI buffers for each near rank
!

!$omp do private(k, kk, rk, buf_size, sync_dir, rank_dir, k_dir, nxs, nxe, nys, nye) schedule(static, 1)
do k = 1, domain%bcount
    if (.not. domain%blocks_info(k)%is_inner) then
        sync_dir(1) = _NXP_; k_dir(1) = domain%blocks_info(k)%k_nxp; rank_dir(1) = domain%blocks_info(k)%rank_nxp
        sync_dir(2) = _NXM_; k_dir(2) = domain%blocks_info(k)%k_nxm; rank_dir(2) = domain%blocks_info(k)%rank_nxm
        sync_dir(3) = _NYP_; k_dir(3) = domain%blocks_info(k)%k_nyp; rank_dir(3) = domain%blocks_info(k)%rank_nyp
        sync_dir(4) = _NYM_; k_dir(4) = domain%blocks_info(k)%k_nym; rank_dir(4) = domain%blocks_info(k)%rank_nym

        sync_dir(5) = _NXP_NYP_; k_dir(5) = domain%blocks_info(k)%k_nxp_nyp; rank_dir(5) = domain%blocks_info(k)%rank_nxp_nyp
        sync_dir(6) = _NXP_NYM_; k_dir(6) = domain%blocks_info(k)%k_nxp_nym; rank_dir(6) = domain%blocks_info(k)%rank_nxp_nym
        sync_dir(7) = _NXM_NYP_; k_dir(7) = domain%blocks_info(k)%k_nxm_nyp; rank_dir(7) = domain%blocks_info(k)%rank_nxm_nyp
        sync_dir(8) = _NXM_NYM_; k_dir(8) = domain%blocks_info(k)%k_nxm_nym; rank_dir(8) = domain%blocks_info(k)%rank_nxm_nym

        do kk = 1, 8
            ! Get rank of proc (rk) for transfer in current direction. It is near rank.
            if (rank_dir(kk) >= 0) then
                if (rank_dir(kk) /= mpp_rank) then
                    rk = sync_map_rank(rank_dir(kk))
                    ! Set local block number (local number is relative to rk) and direction of transfer (direction is relative to rk)
                    _SYNC_SEND_BUF_(sync_buf_pos(kk, k), rk, sync_tag) = 10*k_dir(kk) + get_inverse_dir(sync_dir(kk))
                    !_SYNC_SEND_BUF_(, rk, sync_tag) = get_inverse_dir(sync_dir(kk))
                    !_SYNC_SEND_BUF_(+ 1, rk, sync_tag) = k_dir(kk)

                    ! Set boundary points
                    call get_boundary_points_of_block(domain, k, sync_dir(kk), nxs, nxe, nys, nye)

                    ! Pack buffer
                    buf_size = (nxe - nxs + 1)*(nye - nys + 1)
                    _SYNC_SEND_BUF_(sync_buf_pos(kk, k) + 1 : sync_buf_pos(kk, k) + buf_size , rk, sync_tag) = reshape(data2d%block(k)%field(nxs : nxe, nys : nye), shape=(/buf_size/))
                endif
            endif
        enddo
    endif
enddo
!$omp end do
! Implicit Barrier

!$omp master

call end_timer(t_local)
mpp_time_sync_pack_mpi = mpp_time_sync_pack_mpi + t_local

! Send/Recv MPI buffers for each near rank
!

call start_timer(t_local)

do rk = 1, domain%amount_of_ranks_near
    call mpi_irecv(_SYNC_RECV_BUF_(:, rk, sync_tag), sync_buf_size(rk), _MPI_TYPE_, domain%ranks_near(rk), sync_tag, mpp_cart_comm, reqst, ierr)
    sync_count_send_recv = sync_count_send_recv + 1
    sync_requests(sync_count_send_recv) = reqst
enddo

do rk = 1, domain%amount_of_ranks_near
    call mpi_isend(_SYNC_SEND_BUF_(:, rk, sync_tag), sync_buf_size(rk), _MPI_TYPE_, domain%ranks_near(rk), sync_tag, mpp_cart_comm, reqst, ierr)
    sync_count_send_recv = sync_count_send_recv + 1
    sync_requests(sync_count_send_recv) = reqst
enddo

call end_timer(t_local)
mpp_time_sync_isend_irecv = mpp_time_sync_isend_irecv + t_local

!$omp end master